{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (PIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior for 20% of positive examples: 0.0979\n",
      "Class prior for 30% of positive examples: 0.1457\n",
      "Class prior for 40% of positive examples: 0.1868\n",
      "Class prior for 50% of positive examples: 0.2279\n",
      "F1-score for 20% of positive examples: 0.0000\n",
      "F1-score for 30% of positive examples: 0.0000\n",
      "F1-score for 40% of positive examples: 0.3333\n",
      "F1-score for 50% of positive examples: 0.3077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the data from the MAT-file\n",
    "data = sio.loadmat('/home/kofi/Downloads/Matlab/diabetes.mat')\n",
    "labels = data[\"labels\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into positive and unlabeled sets\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "unlabeled_indices = np.where(labels == 0)[0]\n",
    "X_positive = X[positive_indices, :]\n",
    "X_unlabeled = X[unlabeled_indices, :]\n",
    "\n",
    "# Set the percentage of positive examples to select\n",
    "percentages = [20, 30, 40, 50]\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Split the combined data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Encode the target variable y\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_val = encoder.transform(y_val)\n",
    "\n",
    "    # Train a logistic regression model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the class probabilities for the validation data\n",
    "    y_prob = model.predict_proba(X_val)\n",
    "\n",
    "    # Calculate the class prior using the model-based approach\n",
    "    class_prior = y_prob[:, 1].mean()\n",
    "\n",
    "    print(f\"Class prior for {percent}% of positive examples: {class_prior:.4f}\")\n",
    "\n",
    "    # Perform PIG learning using the class prior\n",
    "    y_pig = y_selected / class_prior\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_pig, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Train a logistic regression model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate the F1-score and store it in the array\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print the F1-scores for each percentage\n",
    "for i, percent in enumerate(percentages):\n",
    "    print(f\"F1-score for {percent}% of positive examples: {f1_scores[i]:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process described in the code below is consistent with the Confidence-Aware Learning (CAL) approach in Positive-Unlabeled (PU) learning, where the class prior estimation method used is the model-based estimation method. In this approach, a logistic regression classifier is trained on a combined dataset of selected positive examples and unlabeled examples. The predicted probabilities for the unlabeled examples are used to estimate the class prior, and an adjusted probability threshold is calculated based on this estimate. The unlabeled examples are then labeled based on this adjusted threshold, and the F1-score is calculated for the labeled examples. The process is repeated for different percentages of positive examples to select, and the results (class prior and F1-score) are printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior for 20% of positive examples: -2.2524\n",
      "F1-score for 20% of positive examples: 0.0000\n",
      "Class prior for 30% of positive examples: -1.8687\n",
      "F1-score for 30% of positive examples: 0.0488\n",
      "Class prior for 40% of positive examples: -1.6212\n",
      "F1-score for 40% of positive examples: 0.1709\n",
      "Class prior for 50% of positive examples: -1.3526\n",
      "F1-score for 50% of positive examples: 0.1389\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the data from the MAT-file\n",
    "data = sio.loadmat('/home/kofi/Downloads/Matlab/diabetes.mat')\n",
    "labels = data[\"labels\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into positive and unlabeled sets\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "unlabeled_indices = np.where(labels == 0)[0]\n",
    "X_positive = X[positive_indices, :]\n",
    "X_unlabeled = X[unlabeled_indices, :]\n",
    "\n",
    "# Set the percentage of positive examples to select\n",
    "percentages = [20, 30, 40, 50]\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Train a logistic regression classifier on the combined dataset\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    # Calculate the predicted probabilities for the unlabeled examples\n",
    "    proba_unlabeled = model.predict_proba(X_unlabeled)[:, 1]\n",
    "\n",
    "    # Calculate the class prior using the model-based approach\n",
    "    class_prior = model.intercept_[0]\n",
    "\n",
    "    print(f\"Class prior for {percent}% of positive examples: {class_prior:.4f}\")\n",
    "\n",
    "    # Adjust the probability threshold based on the class prior\n",
    "    threshold = class_prior / (1 - class_prior)\n",
    "\n",
    "    # Label the unlabeled examples based on the adjusted probability threshold\n",
    "    y_pred = (proba_unlabeled >= threshold).astype(int)\n",
    "\n",
    "    # Create an array with labels for all positive examples\n",
    "    y_true_all = np.concatenate([np.ones(X_positive.shape[0]), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Select the labels for the selected positive examples\n",
    "    y_true = y_true_all[selected_indices]\n",
    "\n",
    "    # Predict the labels for all positive examples\n",
    "    y_pred_all = np.concatenate([model.predict(X_positive), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Select the predictions for the selected positive examples\n",
    "    y_pred = y_pred_all[selected_indices]\n",
    "\n",
    "    # Calculate the F1-score for the labeled examples\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    # Add the F1-score to the array\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1-score for {percent}% of positive examples: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified class prior for 50% of positive examples: 0.5898\n",
      "F1-score for 50% of positive examples: 0.3952\n"
     ]
    }
   ],
   "source": [
    "#Initialize the alpha value\n",
    "alpha = 0.5\n",
    "\n",
    "#Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "# Select a random subset of positive examples\n",
    "selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "X_selected = X_positive[selected_indices]\n",
    "y_selected = np.ones(num_examples)\n",
    "\n",
    "# Combine the selected positive examples with the unlabeled examples\n",
    "X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "# Calculate the empirical class frequency in the labeled examples\n",
    "p_labeled = np.mean(y_selected)\n",
    "\n",
    "# Calculate the class prior using the model-based approach\n",
    "model = LogisticRegression()\n",
    "model.fit(X_combined, y_combined)\n",
    "p_unlabeled = model.predict_proba(X_unlabeled)[:,1].mean()\n",
    "\n",
    "# Calculate the modified class prior\n",
    "p_c = (1 - alpha) * p_labeled + alpha * p_unlabeled\n",
    "\n",
    "print(f\"Modified class prior for {percent}% of positive examples: {p_c:.4f}\")\n",
    "\n",
    "# Train a logistic regression model with the modified class prior\n",
    "model = LogisticRegression(class_weight={1: p_c, 0: 1 - p_c})\n",
    "model.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels for all positive examples\n",
    "y_pred_all = np.concatenate([model.predict(X_positive), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "# Select the predictions for the selected positive examples\n",
    "y_pred = y_pred_all[selected_indices]\n",
    "\n",
    "# Create an array with labels for all positive examples\n",
    "y_true_all = np.concatenate([np.ones(X_positive.shape[0]), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "# Select the labels for the selected positive examples\n",
    "y_true = y_true_all[selected_indices]\n",
    "\n",
    "# Calculate the F1-score for the labeled examples\n",
    "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Add the F1-score to the array\n",
    "f1_scores.append(f1)\n",
    "\n",
    "print(f\"F1-score for {percent}% of positive examples: {f1:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kofi/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified class prior for 20% of positive examples: 0.5000\n",
      "F1-score for 20% of positive examples: 0.0000\n",
      "\n",
      "Modified class prior for 30% of positive examples: 0.6500\n",
      "F1-score for 30% of positive examples: 0.2418\n",
      "\n",
      "Modified class prior for 40% of positive examples: 0.8167\n",
      "F1-score for 40% of positive examples: 0.7910\n",
      "\n",
      "Modified class prior for 50% of positive examples: 0.8000\n",
      "F1-score for 50% of positive examples: 0.8498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the alpha value\n",
    "alpha = 0.5\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Calculate the empirical class frequency in the labeled examples\n",
    "    p_labeled = np.mean(y_selected)\n",
    "\n",
    "    # Calculate the class prior using cross-validation\n",
    "    model = LogisticRegression()\n",
    "    class_prior_scores = cross_val_score(model, X_combined, y_combined, cv=5, scoring=\"precision\")\n",
    "\n",
    "    # Calculate the average class prior across all folds\n",
    "    p_unlabeled = np.mean(class_prior_scores)\n",
    "\n",
    "    # Calculate the modified class prior\n",
    "    p_c = (1 - alpha) * p_labeled + alpha * p_unlabeled\n",
    "\n",
    "    print(f\"Modified class prior for {percent}% of positive examples: {p_c:.4f}\")\n",
    "\n",
    "    # Train a logistic regression model with the modified class prior\n",
    "    model = LogisticRegression(class_weight={1: p_c, 0: 1 - p_c})\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    # Predict the labels for all positive examples\n",
    "    y_pred_all = np.concatenate([model.predict(X_positive), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Select the predictions for the selected positive examples\n",
    "    y_pred = y_pred_all[selected_indices]\n",
    "\n",
    "    # Create an array with labels for all positive examples\n",
    "    y_true_all = np.concatenate([np.ones(X_positive.shape[0]), np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Select the labels for the selected positive examples\n",
    "    y_true = y_true_all[selected_indices]\n",
    "\n",
    "    # Calculate the F1-score for the labeled examples\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    # Add the F1-score to the array\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"F1-score for {percent}% of positive examples: {f1:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
