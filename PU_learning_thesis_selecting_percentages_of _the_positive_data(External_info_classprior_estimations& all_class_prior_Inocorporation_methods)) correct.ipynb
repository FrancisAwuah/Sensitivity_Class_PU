{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate class prior using external information method with data as preference, we need to compute the proportion of positive examples in the external dataset and use it as an estimate of the class prior for the positive class.\n",
    "\n",
    "Here is how we can modify the existing code to calculate class priors for each percentage of positive examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 20, F1-score: 0.3153526970954357, Class prior: 0.0962977587244284\n",
      "Percentage: 30, F1-score: 0.34285714285714286, Class prior: 0.13829424842226046\n",
      "Percentage: 40, F1-score: 0.43988269794721413, Class prior: 0.17656078673245612\n",
      "Percentage: 50, F1-score: 0.4675324675324675, Class prior: 0.21157316272965881\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Load the data from the MAT-file\n",
    "data = sio.loadmat('/home/kofi/Downloads/Matlab/diabetes.mat')\n",
    "labels = data[\"labels\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into positive and unlabeled sets\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "unlabeled_indices = np.where(labels == 0)[0]\n",
    "X_positive = X[positive_indices, :]\n",
    "X_unlabeled = X[unlabeled_indices, :]\n",
    "\n",
    "# Set the percentage of positive examples to select\n",
    "percentages = [20, 30, 40, 50]\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_positive_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_positive_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_positive_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Calculate the class prior using the external information method\n",
    "    external_info = np.sum(labels) / len(labels)\n",
    "    class_prior = (num_positive_examples + external_info) / (X_combined.shape[0] + 1)\n",
    "\n",
    "    # Apply Preprocessing with Incorporating the Class Prior (PIG) method\n",
    "    if len(np.unique(y_combined)) > 1:  # check if the data contains at least two classes\n",
    "        clf = LogisticRegression(penalty='none', solver='lbfgs')\n",
    "        clf.fit(X_combined, y_combined, sample_weight=np.where(y_combined == 1, 1/class_prior, 1/(1-class_prior)))\n",
    "        y_pred = clf.predict(X_combined)\n",
    "        f1 = f1_score(y_combined, y_pred)\n",
    "    else:\n",
    "        f1 = np.nan\n",
    "\n",
    "    # Append the F1-score to the array\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print the F1-score and class prior for this percentage\n",
    "    print(f\"Percentage: {percent}, F1-score: {f1}, Class prior: {class_prior}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modified code, we first calculate the class prior using the external information method with data as preference by dividing the number of positive examples in the dataset by the total number of examples. We then print the class prior.\n",
    "\n",
    "Next, we iterate over the selected percentages and for each percentage, we select a random subset of positive examples, combine them with the unlabeled examples, train a logistic regression model, and evaluate its performance using F1-score with 5-fold cross-validation. We also print the percentage, F1-score, and class prior for each iteration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate the calculated class priors using the PIG (Positive and Unlabeled with Information on the prior of the positive class) method and calculate the accuracy using F1-score, we can modify the existing code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 20, F1-score: 0.36036036036036034, Class prior: 0.0962977587244284\n",
      "Percentage: 30, F1-score: 0.37894736842105264, Class prior: 0.13829424842226046\n",
      "Percentage: 40, F1-score: 0.4417910447761194, Class prior: 0.17656078673245612\n",
      "Percentage: 50, F1-score: 0.451948051948052, Class prior: 0.21157316272965881\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING (PIG)\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the data from the MAT-file\n",
    "data = sio.loadmat('/home/kofi/Downloads/Matlab/diabetes.mat')\n",
    "labels = data[\"labels\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into positive and unlabeled sets\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "unlabeled_indices = np.where(labels == 0)[0]\n",
    "X_positive = X[positive_indices, :]\n",
    "X_unlabeled = X[unlabeled_indices, :]\n",
    "\n",
    "# Set the percentage of positive examples to select\n",
    "percentages = [20, 30, 40, 50]\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_positive_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_positive_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_positive_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Calculate the class prior using the external information method\n",
    "    external_info = np.sum(labels) / len(labels)\n",
    "    class_prior = (num_positive_examples + external_info) / (X_combined.shape[0] + 1)\n",
    "\n",
    "    # Apply Preprocessing with Incorporating the Class Prior (PIG) method\n",
    "    if len(np.unique(y_combined)) > 1:  # check if the data contains at least two classes\n",
    "        clf = LogisticRegression(penalty='none', solver='lbfgs')\n",
    "        clf.fit(X_combined, y_combined, sample_weight=np.where(y_combined == 1, 1/class_prior, 1/(1-class_prior)))\n",
    "        y_pred = clf.predict(X_combined)\n",
    "        f1 = f1_score(y_combined, y_pred)\n",
    "    else:\n",
    "        f1 = np.nan\n",
    "\n",
    "    # Append the F1-score to the array\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print the F1-score and class prior for this percentage\n",
    "    print(f\"Percentage: {percent}, F1-score: {f1}, Class prior: {class_prior}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modified code, we first calculate the class prior using the external information method with data as preference as before.\n",
    "\n",
    "Next, we iterate over the selected percentages and for each percentage, we select a random subset of positive examples, combine them with the unlabeled examples, and incorporate the class prior using the PIG method. To do this, we first compute the prior as the percentage of positive examples, and then we scale the positive examples' labels by dividing by the class prior and multiplying by the estimated prior. This operation adjusts the weight of the positive examples in the model based on the prior knowledge of the positive class prevalence.\n",
    "\n",
    "We then train a logistic regression model using the PIG-adjusted labels and evaluate its performance using F1-score with 5-fold cross-validation. We also print the percentage and F1-score for each iteration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the postprocessing (CAL) method under class prior incorporation, we can use the CalibratedClassifierCV method from scikit-learn. This method performs a cross-validation calibration of the classifier to improve the predicted probabilities. We will use the same logistic regression classifier as before and incorporate the calculated class priors.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 20, F1-score: 0.000, Class prior: 0.212\n",
      "Percentage: 30, F1-score: 0.000, Class prior: 0.212\n",
      "Percentage: 40, F1-score: 0.000, Class prior: 0.212\n",
      "Percentage: 50, F1-score: 0.000, Class prior: 0.212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize the logistic regression classifier\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    def compute_pig_weights(y, class_prior):\n",
    "    #Computes the PIG (Prior Ignorance Guess) weights for each sample in y.\n",
    "\n",
    "    # Parameters:\n",
    "        #y (array-like): The target variable.\n",
    "        #class_prior (float): The prior probability of the positive class.\n",
    "\n",
    "    #Returns:\n",
    "        #array-like: The PIG weights for each sample in y.\n",
    "\n",
    "    # Compute the PIG weights\n",
    "        pos_weight = class_prior\n",
    "        neg_weight = 1 - class_prior\n",
    "        weights = np.zeros_like(y)\n",
    "        weights[y == 1] = pos_weight\n",
    "        weights[y == 0] = neg_weight\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    # Perform class prior incorporation using PIG method\n",
    "    clf.fit(X_train, y_train, sample_weight=compute_pig_weights(y_train, class_prior))\n",
    "\n",
    "    # Perform class prior incorporation using CAL method\n",
    "    clf_calibrated = CalibratedClassifierCV(clf, cv=5, method='sigmoid')\n",
    "    clf_calibrated.fit(X_train, y_train, sample_weight=compute_pig_weights(y_train, class_prior))\n",
    "\n",
    "    # Evaluate the classifier on the test set using F1-score\n",
    "    y_pred = clf_calibrated.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"Percentage: {}, F1-score: {:.3f}, Class prior: {:.3f}\".format(percent, f1, class_prior))\n",
    "\n",
    "    # Append the F1-score to the array\n",
    "    f1_scores.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior: 0.2054\n",
      "F1-score with PIG method: 0.0000\n",
      "F1-score with CAL method: 0.0000\n",
      "Modified class prior (alpha=0.1): 0.2054\n",
      "F1-score with modified class priors (alpha=0.1): 0.0000\n",
      "Modified class prior (alpha=0.3): 0.2054\n",
      "F1-score with modified class priors (alpha=0.3): 0.0000\n",
      "Modified class prior (alpha=0.5): 0.2054\n",
      "F1-score with modified class priors (alpha=0.5): 0.0000\n",
      "Modified class prior (alpha=0.7): 0.2054\n",
      "F1-score with modified class priors (alpha=0.7): 0.0000\n",
      "Modified class prior (alpha=0.9): 0.2054\n",
      "F1-score with modified class priors (alpha=0.9): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define function to compute modified class priors\n",
    "def compute_modified_class_priors(y_labeled, y_unlabeled, alpha):\n",
    "    labeled_freq = np.mean(y_labeled)\n",
    "    unlabeled_freq = np.mean(y_unlabeled)\n",
    "    return (1 - alpha) * labeled_freq + alpha * unlabeled_freq\n",
    "\n",
    "# Split labeled and unlabeled examples\n",
    "idx_labeled = (y_train != -1)\n",
    "idx_unlabeled = (y_train == -1)\n",
    "X_labeled_train = X_train[idx_labeled]\n",
    "y_labeled_train = y_train[idx_labeled]\n",
    "X_unlabeled_train = X_train[idx_unlabeled]\n",
    "\n",
    "# Compute class priors\n",
    "class_prior = np.mean(y_labeled_train)\n",
    "print(\"Class prior: {:.4f}\".format(class_prior))\n",
    "\n",
    "# Train logistic regression model with class prior incorporation using PIG method\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(X_labeled_train, y_labeled_train, sample_weight=compute_pig_weights(y_labeled_train, class_prior))\n",
    "\n",
    "# Perform class prior incorporation using CAL method\n",
    "clf_calibrated = CalibratedClassifierCV(clf, cv=5, method='sigmoid')\n",
    "clf_calibrated.fit(X_labeled_train, y_labeled_train, sample_weight=compute_pig_weights(y_labeled_train, class_prior))\n",
    "\n",
    "# Compute F1-score for both methods\n",
    "y_pred_pig = clf.predict(X_test)\n",
    "y_pred_cal = clf_calibrated.predict(X_test)\n",
    "f1_pig = f1_score(y_test, y_pred_pig)\n",
    "f1_cal = f1_score(y_test, y_pred_cal)\n",
    "print(\"F1-score with PIG method: {:.4f}\".format(f1_pig))\n",
    "print(\"F1-score with CAL method: {:.4f}\".format(f1_cal))\n",
    "\n",
    "# Train logistic regression model with class prior incorporation using modified class priors\n",
    "alpha_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for alpha in alpha_values:\n",
    "    class_prior = compute_modified_class_priors(y_labeled_train, y_train, alpha)\n",
    "    print(\"Modified class prior (alpha={}): {:.4f}\".format(alpha, class_prior))\n",
    "    clf = LogisticRegression(max_iter=10000, class_weight={0: 1-class_prior, 1: class_prior})\n",
    "    clf.fit(X_labeled_train, y_labeled_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1-score with modified class priors (alpha={}): {:.4f}\".format(alpha, f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
