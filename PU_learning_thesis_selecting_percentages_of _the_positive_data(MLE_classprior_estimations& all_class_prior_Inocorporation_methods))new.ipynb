{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prior for 20% of positive examples: 1.0000\n",
      "Class prior for 30% of positive examples: 1.0000\n",
      "Class prior for 40% of positive examples: 1.0000\n",
      "Class prior for 50% of positive examples: 1.0000\n",
      "F1-score for 20% of positive examples: 0.0000\n",
      "F1-score for 30% of positive examples: 0.1111\n",
      "F1-score for 40% of positive examples: 0.0000\n",
      "F1-score for 50% of positive examples: 0.1667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the data from the MAT-file\n",
    "data = sio.loadmat('/home/kofi/Downloads/Matlab/diabetes.mat')\n",
    "labels = data[\"labels\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into positive and unlabeled sets\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "unlabeled_indices = np.where(labels == 0)[0]\n",
    "X_positive = X[positive_indices, :]\n",
    "X_unlabeled = X[unlabeled_indices, :]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the percentage of positive examples to select\n",
    "percentages = [20, 30, 40, 50]\n",
    "\n",
    "# Initialize the F1-score array\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Calculate the class prior for the selected positive examples\n",
    "    class_prior = np.mean(y_selected)\n",
    "\n",
    "    # Perform PIG learning using the class prior\n",
    "    y_pig = y_selected / class_prior\n",
    "    print(f\"Class prior for {percent}% of positive examples: {class_prior:.4f}\")\n",
    "    \n",
    "    # Combine the selected positive examples with the unlabeled examples\n",
    "    X_combined = np.concatenate([X_selected, X_unlabeled])\n",
    "    y_combined = np.concatenate([y_pig, np.zeros(X_unlabeled.shape[0])])\n",
    "\n",
    "    # Split the combined data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a logistic regression model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate the F1-score and store it in the array\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print the F1-scores for each percentage\n",
    "for i, percent in enumerate(percentages):\n",
    "    print(f\"F1-score for {percent}% of positive examples: {f1_scores[i]:.4f}\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code modifies the previous code to perform PIG learning using the calculated class prior. It divides the selected positive examples by the class prior to obtain the PIG labels. It then combines the PIG labels with the unlabeled examples, splits the combined data into training and validation sets, trains a logistic regression model on the training data, makes predictions on the validation data, and calculates the F1-score.\n",
    "\n",
    "The code stores the F1-scores in an array and prints them for each percentage. The F1-score is a measure of the model's accuracy that takes into account both precision and recall, and it is commonly used in binary classification problems.\n",
    "\n",
    "Note that the code assumes that the positive examples are labeled with 1 and the unlabeled examples are labeled with 0. If this is not the case, you may need to modify the code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform post-processing using CAL, we need to first train a classifier on the training set using PU learning with PIG. We will then use the trained classifier to predict the probabilities of each sample in the validation set being positive. We will then apply the class prior to these probabilities to obtain the corrected probabilities. Finally, we will use a threshold to convert these probabilities into binary labels and evaluate the F1-score.\n",
    "\n",
    "Here's the code to perform post-processing using CAL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for 20% of positive examples using CAL: 0.0000, Class prior: 1.0000\n",
      "F1-score for 30% of positive examples using CAL: 0.0667, Class prior: 1.0000\n",
      "F1-score for 40% of positive examples using CAL: 0.1250, Class prior: 1.0000\n",
      "F1-score for 50% of positive examples using CAL: 0.2632, Class prior: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold for converting probabilities to binary labels\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize empty lists to store the F1-scores and class priors for each percentage of positive examples\n",
    "f1_scores_cal = []\n",
    "class_priors = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Calculate the class prior for the selected positive examples\n",
    "    class_prior = np.mean(y_selected)\n",
    "    class_priors.append(class_prior)\n",
    "\n",
    "    # Perform PIG learning on the selected positive examples and the unlabeled examples\n",
    "    y_combined_pig_new = np.zeros(X_combined.shape[0])\n",
    "    y_combined_pig_new[:num_examples] = 1\n",
    "    pig_classifier = LogisticRegression(random_state=42)\n",
    "    pig_classifier.fit(X_combined, y_combined_pig_new)\n",
    "\n",
    "    # Use the trained PIG classifier to predict the probabilities of each sample in the validation set being positive\n",
    "    y_val_prob = pig_classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Apply the class prior to the probabilities to obtain the corrected probabilities\n",
    "    y_val_prob_cal = y_val_prob / class_prior\n",
    "\n",
    "    # Convert the probabilities to binary labels using the threshold\n",
    "    y_val_pred = (y_val_prob_cal >= threshold).astype(int)\n",
    "\n",
    "    # Calculate the F1-score\n",
    "    f1_score_cal = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    # Print the F1-score\n",
    "    print(f\"F1-score for {percent}% of positive examples using CAL: {f1_score_cal:.4f}, Class prior: {class_prior:.4f}\")\n",
    "\n",
    "    # Append the F1-score to the list of F1-scores\n",
    "    f1_scores_cal.append(f1_score_cal)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets a threshold for converting probabilities to binary labels, initializes an empty list to store F1-scores, and iterates over selected percentages of positive examples. For each percentage, it selects a random subset of positive examples, calculates the class prior, performs PIG learning on the selected positive examples and the unlabeled examples, uses the trained PIG classifier to predict the probabilities of each sample in the validation set being positive, applies the class prior to the probabilities to obtain the corrected probabilities, converts the probabilities to binary labels using the threshold, calculates the F1-score, prints the F1-score, and appends the F1-score to the list of F1-scores.\n",
    "\n",
    "This code implements a method called \"Confidence-Aware Learning\" (CAL), which is a variant of Pseudo-Labeling that incorporates a correction factor based on the estimated class prior. CAL is designed to address the issue of class imbalance that can occur when training a classifier on a dataset with a small number of positive examples.\n",
    "\n",
    "Overall, this code evaluates the performance of the PIG classifier with the CAL correction factor on a validation set for different percentages of positive examples in the training set.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will print the F1-score for each percentage of positive examples using CAL and store the F1-scores in the f1_scores_cal list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform method modification using the Bekker and Davis (BD) method, we need to first train a classifier on the training set using PU learning with PIG. We will then use the trained classifier to predict the probabilities of each sample in the validation set being positive. We will then apply the class prior to these probabilities to obtain the corrected probabilities. We will then modify these probabilities using the BD method, which involves scaling the probabilities by a factor that depends on the class prior and a tuning parameter alpha. Finally, we will use a threshold to convert these probabilities into binary labels and evaluate the F1-score.\n",
    "\n",
    "Here's the code to perform method modification using the BD method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for 20% of positive examples using BD: 0.3718, class prior: 1.0000\n",
      "F1-score for 30% of positive examples using BD: 0.3718, class prior: 1.0000\n",
      "F1-score for 40% of positive examples using BD: 0.3718, class prior: 1.0000\n",
      "F1-score for 50% of positive examples using BD: 0.3718, class prior: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold for converting probabilities to binary labels\n",
    "threshold = 0.5\n",
    "\n",
    "# Set the tuning parameter alpha\n",
    "alpha = 1\n",
    "\n",
    "# Initialize an empty list to store the F1-scores for each percentage of positive examples\n",
    "f1_scores_bd = []\n",
    "\n",
    "# Iterate over the selected percentages\n",
    "for percent in percentages:\n",
    "    # Calculate the number of positive examples to select\n",
    "    num_examples = int(X_positive.shape[0] * percent / 100)\n",
    "\n",
    "    # Select a random subset of positive examples\n",
    "    selected_indices = np.random.choice(X_positive.shape[0], size=num_examples, replace=False)\n",
    "    X_selected = X_positive[selected_indices]\n",
    "    y_selected = np.ones(num_examples)\n",
    "\n",
    "    # Calculate the class prior for the selected positive examples\n",
    "    class_prior = np.mean(y_selected)\n",
    "\n",
    "    # Perform PIG learning on the selected positive examples and the unlabeled examples\n",
    "    y_combined_pig = np.concatenate([y_selected, np.zeros(X_unlabeled.shape[0])])\n",
    "    X_combined_pig = np.concatenate([X_selected, X_unlabeled])\n",
    "    pig_classifier = LogisticRegression(random_state=42)\n",
    "    pig_classifier.fit(X_combined_pig, y_combined_pig)\n",
    "\n",
    "\n",
    "    # Use the trained PIG classifier to predict the probabilities of each sample in the validation set being positive\n",
    "    y_val_prob = pig_classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Apply the class prior to the probabilities to obtain the corrected probabilities\n",
    "    y_val_prob_cal = y_val_prob / class_prior\n",
    "\n",
    "    # Modify the probabilities using the BD method\n",
    "    y_val_prob_bd = y_val_prob_cal / (y_val_prob_cal + (1 - class_prior) / class_prior * (1 - y_val_prob_cal) ** alpha)\n",
    "\n",
    "    # Convert the probabilities to binary labels using the threshold\n",
    "    y_val_pred = (y_val_prob_bd >= threshold).astype(int)\n",
    "\n",
    "    # Calculate the F1-score\n",
    "    f1_score_bd = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    # Print the F1-score and class prior\n",
    "    print(f\"F1-score for {percent}% of positive examples using BD: {f1_score_bd:.4f}, class prior: {class_prior:.4f}\")\n",
    "\n",
    "    # Append the F1-score to the list of F1-scores\n",
    "    f1_scores_bd.append(f1_score_bd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the F1-scores for each percentage of positive examples using the BD method for PIG. It uses the same data and parameters as the previous code, and applies the BD method to modify the probabilities obtained from the PIG classifier. It also prints the F1-score for each percentage of positive examples and stores them in a list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
